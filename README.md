# Test the use of vllm
vLLM is a fast and easy-to-use library for LLM inference and serving.  
https://docs.vllm.ai/en/latest/getting_started/quickstart.html  

Run from within Google Colab using the runtime set to T4 GPU.
